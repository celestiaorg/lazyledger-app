# PBBT Messages and Validation Logic

At a high level, all flavors of PBBT have four message types. `Commitment`,
`Have`, `Want`, and `Data`.

## Commitment

```proto
message TxMetaData {
  bytes  hash  = 1;
  uint32 start = 2;
  uint32 end   = 3;
}

// CompactBlock commits to the transaction included in a proposal.
message CompactBlock {
  int64               height    = 1;
  int32               round     = 2;
  bytes               bp_hash   = 3;
  repeated TxMetaData blobs     = 4;
  bytes               signature = 5;
}
```

The compact block is signed over by the proposer, and verified by converting to
signbytes, and using the proposer's public key to verify the included signature.

> Note: This siganture is separate from the proposal signature as it is purely
> related to block propagation, and not meant to be part of the proposal. This
> allows for block propagation to be backwards compatible with older
> implementations.

The `TxMetaData` contains the hash of the PFB for the blob transaction that it
commits to, alongside the `start` and `end`. `start` is the inclusive index of
the starting byte in the protobuf encoded block. `end` depicts the last byte
occupied by the blob transaction.

The `pbbt_root` is generated by taking the merkle root over of each of the blob
transactions in `BlobMetaData` and `Have` messasges.

Verification:

- The signature MUST be valid using the sign bytes of the compact block and the public key of the expected proposer for that height and
  round.

## Have

```protobuf=
message HaveParts {
  bytes                   hash   = 1;
  int64                   height = 2;
  int32                   round  = 3;
  tendermint.crypto.Proof proof  = 4 [(gogoproto.nullable) = false];
}
```

Verification:

- The merkle proof MUST be verified using the roots included in the
  `CompactBlock` for that height and round. If the data is parity data, then it
  MUST use the `parity_root`, if the data is original block data, then it MUST
  use the `PartSetHeaderRoot`.

### Want

```protobuf
message WantParts {
  tendermint.libs.bits.BitArray parts  = 1 [(gogoproto.nullable) = false];
  int64                         height = 2;
  int32                         round  = 3;
}
```

## Data

```protobuf
message RecoveryPart {
  int64  height = 1;
  int32  round  = 2;
  uint32 index  = 3;
  bytes  data   = 4;
}
```

Verification

- The hash of the bytes in the data field MUST match that of the `Have` message.

### Parity Data

Parity data is required for all practical broadcast trees. This becomes
problematic mainly due to the requirement that transactions downloaded before
the block is created need to be used during recovery. Using erasure encoding
means that the data must be chunked in an even size. All transactions in that
chunk must have been downloaded in order to use it alongside parity data to
reconstruct the block. Most scenarios would likely be fine, however it would be
possible for a node to have downloaded a large portion of the block, but have no
complete parts, rendering all of the parity data useless. The way to fix this
while remaining backwards compatible is to still commit over and propagate
parts, but to erasure encode smaller chunks of those parts, aka `SubParts`.

```go
const (
	SubPartsPerPart uint32 = 32
	SubPartSize            = BlockPartSizeBytes / SubPartsPerPart
)

type Part struct {
	Index uint32            `json:"index"`
	Bytes cmtbytes.HexBytes `json:"bytes"`
	Proof merkle.Proof      `json:"proof"`
}

// SubPart is a portion of a part and block that is used for generating parity
// data.
type SubPart struct {
	Index uint32            `json:"index"`
	Bytes cmtbytes.HexBytes `json:"bytes"`
}

// SubPart breaks a block part into smaller equal sized subparts.
func (p *Part) SubParts() []SubPart {
  sps := make([]SubPart, SubPartsPerPart)
  for i := uint32(0); i < SubPartsPerPart; i++ {
    sps[i] = SubPart{
      Index: uint32(i),
      Bytes: p.Bytes[i*SubPartSize : (i+1)*SubPartSize],
    }
  }
  return sps
}

func PartFromSubParts(index uint32, sps []SubPart) *Part {
  if len(sps) != int(SubPartsPerPart) {
    panic(fmt.Sprintf("invalid number of subparts: %d", len(sps)))
  }
  b := make([]byte, 0, BlockPartSizeBytes)
  for _, sp := range sps {
    b = append(b, sp.Bytes...)
  }
  return &Part{
    Index: index,
    Bytes: b,
  }
}
```
